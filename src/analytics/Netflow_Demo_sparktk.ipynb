{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries and connect to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import socket,struct\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time as time\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "#Unique prefix to make sure my names don't conflict with yours\n",
    "MY_SUFFIX = \"_\" + os.getcwd().split('/')[-1] \n",
    "\n",
    "print \"MY_SUFFIX =\",MY_SUFFIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the sparktk and catalog libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sparktk\n",
    "import tap_catalog\n",
    "from sparktk import TkContext\n",
    "from tap_catalog import DataCatalog\n",
    "\n",
    "print \"SparkTK installation path = %s\" % (sparktk.__path__)\n",
    "\n",
    "tc = TkContext(master='yarn-client', extra_conf={\"spark.yarn.am.memory\":\"3712m\", \n",
    "                                                   \"spark.executor.memory\":\"3712m\", \n",
    "                                                   \"spark.yarn.driver.memoryOverhead\": \"384\",\n",
    "                                                   \"spark.yarn.executor.memoryOverhead\": \"384\",\n",
    "                                                   \"spark.driver.cores\": \"1\",\n",
    "                                                   \"spark.executor.cores\":\"1\",\n",
    "                                                   \"spark.shuffle.io.preferDirectBufs\":\"false\",\n",
    "                                                   \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "                                                   \"spark.shuffle.service.enabled\": \"true\",\n",
    "                                                   \"spark.sql.shuffle.partitions\": \"10\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup objects prior to next run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reference = MY_SUFFIX \n",
    "score = reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = \"hdfs://nameservice1/org/29ace093-e11f-4f0b-b254-3f8e973476e5/brokers/userspace/694b3da9-c21a-4063-bf16-e072ac47f881/bae3a89e-eaad-4a8b-b1b9-f42e1d1426cc/000000_1\"\n",
    "week2_nf_schema=[(\"TimeSeconds\", float),\n",
    "                 (\"tstart\", str),\n",
    "                 (\"dateTimeStr\", float),\n",
    "                 (\"protocol\", str),\n",
    "                 (\"proto\", str),\n",
    "                 (\"src\", str),\n",
    "                 (\"dst\", str),\n",
    "                 (\"sport\", int),\n",
    "                 (\"dport\", int),\n",
    "                 (\"flag\", int),\n",
    "                 (\"fwd\", int),\n",
    "                 (\"tdur\", int),\n",
    "                 (\"firstSeenSrcPayloadBytes\", int),\n",
    "                 (\"firstSeenDestPayloadBytes\", int),\n",
    "                 (\"ibyt\", int),\n",
    "                 (\"obyt\", int),\n",
    "                 (\"ipkt\", int),\n",
    "                 (\"opkt\", int),\n",
    "                 (\"recordForceOut\", int)]\n",
    "\n",
    "real_netflow = tc.frame.import_csv(path=dataset, schema=week2_nf_schema, header=True)\n",
    "\n",
    "real_netflow.inspect(wrap=10, round=4, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Add Columns for date/time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_time(row):\n",
    "    x = row.tstart.split(\" \")\n",
    "    date = x[0]\n",
    "    time = x[1]\n",
    "    y = time.split(\":\")\n",
    "    hour = float(y[0])\n",
    "    minute = float(y[1])\n",
    "    numeric_time = hour + minute/60.0\n",
    "    return [date, time, hour, minute, numeric_time]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_netflow.add_columns(get_time, [('date',str), \n",
    "                                    ('time',str), \n",
    "                                    ('hour', float), \n",
    "                                    ('minute', float), \n",
    "                                    ('numeric_time', float)])\n",
    "\n",
    "real_netflow.inspect(wrap=10, round=4, width=100,\n",
    "                     columns=['date', 'time', 'hour', 'minute', 'numeric_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Graph for computing graph statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create edge dataframe\n",
    "network_edges = real_netflow.group_by(['src', 'dst'], tc.agg.count)\n",
    "# Create vertex dataframe\n",
    "src_frame = network_edges.copy()\n",
    "src_frame.drop_columns('dst')\n",
    "src_frame.rename_columns({\"src\":\"id\"})\n",
    "dst_frame = network_edges.copy()\n",
    "dst_frame.drop_columns('src')\n",
    "dst_frame.rename_columns({\"dst\":\"id\"})\n",
    "src_frame.append(dst_frame)\n",
    "network_vertices = src_frame.group_by(['id'])\n",
    "\n",
    "# Create a graph\n",
    "network_graph = tc.graph.create(network_vertices,network_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute bytes in and bytes out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bytes_out = real_netflow.group_by(['src'], tc.agg.count, {'ibyt': tc.agg.sum})\n",
    "bytes_out.rename_columns({'ibyt_SUM': 'obyt_SUM', 'count': 'outgoing_connections'})\n",
    "print bytes_out.inspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bytes_in = real_netflow.group_by(['dst'], {'ibyt': tc.agg.sum})\n",
    "bytes_in.rename_columns({'dst': 'src'})\n",
    "print bytes_in.inspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bytes_in_out = bytes_in.join_inner(bytes_out, left_on='src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bytes_in_out.add_columns(lambda row: [float(np.log(row.ibyt_SUM)), float(np.log(row.obyt_SUM))], \n",
    "                         [('ln_ibyt_SUM', float), ('ln_obyt_SUM', float)])\n",
    "\n",
    "print bytes_in_out.inspect(wrap=10, round=4, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Weighted/Unweighted Degree Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Compute unweighted degree count:'\n",
    "unweighted_degree_frame = network_graph.degrees('undirected')\n",
    "print unweighted_degree_frame.inspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print 'Compute weighted degree count:'\n",
    "weighted_degree_frame = network_graph.weighted_degrees(edge_weight = 'count')\n",
    "\n",
    "print weighted_degree_frame.inspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Summary frame and Download it to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_degree_frame.rename_columns({'id': 'src', 'degree' : 'weighted_degree'})\n",
    "ip_summary_frame_intermediate = bytes_in_out.join_inner(weighted_degree_frame, left_on='src')\n",
    "\n",
    "print ip_summary_frame_intermediate.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unweighted_degree_frame.rename_columns({'id':'src', 'degree' : 'unweighted_degree'})\n",
    "ip_summary_frame = ip_summary_frame_intermediate.join_inner(unweighted_degree_frame, left_on='src')\n",
    "\n",
    "print ip_summary_frame.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ip_summary_frame.add_columns(lambda row: [row.ibyt_SUM + row.obyt_SUM, \n",
    "                                          float(np.log(row.ibyt_SUM + row.obyt_SUM)),\n",
    "                                          float(np.log(row.unweighted_degree)),\n",
    "                                          float(np.log(row.weighted_degree))], \n",
    "                             [('total_traffic', float),\n",
    "                             ('ln_total_traffic', float),\n",
    "                             ('ln_degree', float),\n",
    "                             ('ln_weighted_degree', float)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip_summary_frame.rename_columns({'src': 'ip'})\n",
    "\n",
    "print ip_summary_frame.inspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip_summary_frame_pd = ip_summary_frame.to_pandas(ip_summary_frame.count())\n",
    "ip_summary_frame_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute histogram bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outgoing_connections_bins = 100\n",
    "ln_ibyt_SUM_bins = 100\n",
    "ln_obyt_SUM_bins = 100\n",
    "ln_total_traffic_bins = 100\n",
    "weighted_degree_bins = 100\n",
    "degree_bins = 100\n",
    "\n",
    "def plot_hist(histogram):\n",
    "    plt.bar(histogram.cutoffs[:-1], histogram.hist, width = histogram.cutoffs[1]-histogram.cutoffs[0])\n",
    "    plt.xlim(min(histogram.cutoffs), max(histogram.cutoffs))\n",
    "\n",
    "histograms = {}\n",
    "histograms['outgoing_connections'] = ip_summary_frame.histogram(\n",
    "    column_name = \"outgoing_connections\", num_bins=outgoing_connections_bins)\n",
    "histograms['ln_ibyt_SUM'] = ip_summary_frame.histogram(column_name = \"ln_ibyt_SUM\", num_bins=ln_ibyt_SUM_bins)\n",
    "histograms['ln_obyt_SUM'] = ip_summary_frame.histogram(column_name = \"ln_obyt_SUM\", num_bins=ln_obyt_SUM_bins)\n",
    "histograms['ln_total_traffic'] = ip_summary_frame.histogram(column_name = \"ln_total_traffic\", num_bins=ln_total_traffic_bins)\n",
    "histograms['ln_weighted_degree'] = ip_summary_frame.histogram(column_name = \"ln_weighted_degree\", num_bins=weighted_degree_bins)\n",
    "histograms['ln_degree'] = ip_summary_frame.histogram(column_name = \"ln_degree\", num_bins=degree_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(histograms['outgoing_connections'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(histograms['ln_ibyt_SUM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(histograms['ln_obyt_SUM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(histograms['ln_total_traffic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(histograms['ln_weighted_degree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(histograms['ln_degree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the scatter_matrix functionality\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "# define colors list, to be used to plot survived either red (=0) or green (=1)\n",
    "colors=['yellow','blue']\n",
    "\n",
    "# make a scatter plot\n",
    "df = pd.DataFrame(ip_summary_frame.take(ip_summary_frame.count(), \n",
    "                                        columns=[\"ln_ibyt_SUM\", \"ln_obyt_SUM\", \"ln_degree\", \"ln_weighted_degree\"]), \n",
    "                  columns=[\"ln_ibyt_SUM\", \"ln_obyt_SUM\", \"ln_degree\", \"ln_weighted_degree\"])\n",
    "\n",
    "scatter_matrix(df, figsize=[20,20],marker='x',\n",
    "               c=df.ln_degree.apply(lambda x: colors[1]))\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train an SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip_summary_frame.add_columns(lambda row: '1', (\"label\", float))\n",
    "ip_summary_frame.inspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVM_model = tc.models.classification.svm.train(ip_summary_frame, 'label', \n",
    "                ['ln_ibyt_SUM', 'ln_obyt_SUM', 'ln_degree', 'ln_weighted_degree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scored_frame = SVM_model.predict(ip_summary_frame, \n",
    "                                 ['ln_ibyt_SUM', 'ln_obyt_SUM', 'ln_degree', 'ln_weighted_degree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scored_frame_group_by = scored_frame.group_by('predicted_label', tc.agg.count)\n",
    "\n",
    "scored_frame_group_by.inspect(wrap=10, round=4, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download scatter frame to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(scored_frame.take(ip_summary_frame.count(), \n",
    "                                    columns=[\"ln_ibyt_SUM\", \"ln_obyt_SUM\", \"ln_degree\", \"ln_weighted_degree\", \"predicted_label\"]), \n",
    "                  columns=[\"ln_ibyt_SUM\", \"ln_obyt_SUM\", \"ln_degree\", \"ln_weighted_degree\", \"predicted_label\"])\n",
    "\n",
    "#scored_frame_pd = scored_frame.download(scored_frame.row_count)\n",
    "# import the scatter_matrix functionality\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "# define colors list, to be used to plot survived either red (=0) or green (=1)\n",
    "colors=['yellow','blue']\n",
    "\n",
    "# make a scatter plot\n",
    "scatter_matrix(df, figsize=[20,20],marker='x',\n",
    "               c=df.predicted_label.apply(lambda x: (colors[1] if x == 1 else colors[0])))\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the Model in MAR format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the trained model to MAR format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SVM_model.export_to_mar(\"hdfs://nameservice1/user/vcap/netflow_svm_model.mar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data Catalog client module from tap_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tap_catalog import DataCatalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an instance of Data Catalog\n",
    "### data_catalog = DataCatalog('TAP_DOMAIN_URI', 'TAP_USERNAME', 'TAP_PASSWORD') # For Scripting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_catalog = DataCatalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_catalog.add(\"hdfs://nameservice1/user/vcap/netflow_svm_model.mar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only run the below lines to remove file/models from HDFS (Cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect HDFS directly using hdfsclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import hdfsclient\n",
    "#from hdfsclient import ls, mkdir, rm, mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ls(\"/user/vcap/*.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup the file from HDFS\n",
    "### (This does not delete from data catalog. Remember to delete it from the Data Catalog UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rm(\"/user/vcap/netflow_svm_model.mar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
